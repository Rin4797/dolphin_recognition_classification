{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9DBE5D5C-AC87-30CA-96DA-F5BC116EDA2B> /opt/homebrew/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <44DEDA27-4DE9-3D4A-8EDE-5AA72081319F> /opt/homebrew/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "f_names = {}\n",
    "for filename in glob('../data/**/*.JPG', recursive=True):  # data содержит папки D850-Vol1 и тд\n",
    "    f_names[filename.split('/')[-1]] = {'path': filename} # \\\\ -> /\n",
    "\n",
    "\n",
    "markup_updated = pd.read_json('../artifacts/markup_updated.json').T.to_dict()\n",
    "\n",
    "areas = []\n",
    "for f in markup_updated:\n",
    "    for m in markup_updated[f]['coco']:\n",
    "        areas.append(m[-1] * m[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import joblib\n",
    "\n",
    "sets_info = joblib.load('../artifacts/sets_info_v3.joblib')\n",
    "save_path = 'preds_all'\n",
    "results = joblib.load('../artifacts/res_8s_slicing_0.45.joblib')\n",
    "\n",
    "fname2index = {j:i for i,j in dict(enumerate(f_names.keys())).items()}\n",
    "index2fname = {j:i for i,j in fname2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(sets_info['train']['names']))\n",
    "print(len(sets_info['val']['names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 297\n",
      "recall 0.871\n",
      "precision 0.616\n",
      "f_score 0.722\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../utils')\n",
    "from metrics import calc_tp_fp_fn, get_true_boxes_format, get_predicted_boxes_format, print_main_metrics\n",
    "\n",
    "confidence_threshold = 0.45\n",
    "min_area = np.quantile(areas, 0.02)\n",
    "max_area=np.quantile(areas, 0.98)\n",
    "score_threshold = 0.8265\n",
    "iou_threshold=0.2\n",
    "\n",
    "detections = get_predicted_boxes_format({i: results[i] for i in results}, fname2index, min_area=min_area, max_area=max_area)\n",
    "detections = [i for i in detections if i[2] > score_threshold]\n",
    "\n",
    "ground_truths = get_true_boxes_format({i: markup_updated[i] for i in markup_updated}, fname2index)\n",
    "\n",
    "tp_list, fp_list, fn = calc_tp_fp_fn(ground_truths, detections, iou_threshold)\n",
    "\n",
    "\n",
    "print_main_metrics(*calc_tp_fp_fn(ground_truths, [i for i in detections if i[2] > score_threshold], iou_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 482/482 [02:32<00:00,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "def classify_crop(crop_img, model=None):\n",
    "    return random()\n",
    "\n",
    "\n",
    "detections_after_classifier = []\n",
    "\n",
    "for detection in tqdm(detections):\n",
    "\n",
    "    img = cv2.imread(f_names[index2fname[detection[0]]]['path'])\n",
    "    x_min, y_min, x_max, y_max = [int(i) for i in detection[3:]]\n",
    "\n",
    "    crop_img = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    \n",
    "    # Вызываем классификатор\n",
    "    if classify_crop(crop_img=crop_img) > 0.5:\n",
    "        detections_after_classifier.append(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 144\n",
      "recall 0.422\n",
      "precision 0.621\n",
      "f_score 0.503\n"
     ]
    }
   ],
   "source": [
    "print_main_metrics(*calc_tp_fp_fn(ground_truths, detections_after_classifier, iou_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform_resize = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class ResNetPad():\n",
    "    def __call__(self, image, shape=(224, 224)):\n",
    "        y_, x_ = shape\n",
    "        y, x = image.size[1], image.size[0]\n",
    "        y_pad = (y_-y)\n",
    "        x_pad = (x_-x)\n",
    "        return np.pad(image, ((y_pad//2, y_pad//2 + y_pad%2),\n",
    "                         (x_pad//2, x_pad//2 + x_pad%2),\n",
    "                         (0, 0)),\n",
    "                      mode = 'constant')\n",
    "\n",
    "data_transform_padding = transforms.Compose([\n",
    "    ResNetPad(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet50_padding = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights)\n",
    "resnet50_resize = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights)\n",
    "resnet50_knn = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights)\n",
    "resnet50_resize_knn = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights)\n",
    "\n",
    "resnet50_padding_dict = torch.load('./resnet50_padding_split.pth', map_location=torch.device('cpu'))\n",
    "resnet50_resize_dict = torch.load('./resnet50_resize_split.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "resnet50_padding.load_state_dict(resnet50_padding_dict)\n",
    "resnet50_resize.load_state_dict(resnet50_resize_dict)\n",
    "resnet50_resize_knn.load_state_dict(resnet50_resize_dict)\n",
    "\n",
    "resnet50_knn.fc = torch.nn.Identity()\n",
    "resnet50_resize_knn.fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "knn = pickle.load(open('./knnpickle_file', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_crop(crop_img, model=None, transform=None):\n",
    "    model.eval()\n",
    "    \n",
    "    im = Image.fromarray(np.uint8(crop_img[:, :, [2, 1, 0]]))\n",
    "    transformed_image = transform(im)\n",
    "    transformed_image = transformed_image.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(transformed_image)\n",
    "    prediction = torch.argmax(outputs, dim=1)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 482/482 [03:08<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "detections_after_classifier = []\n",
    "predictions = []\n",
    "\n",
    "for detection in tqdm(detections):\n",
    "\n",
    "    img = cv2.imread(f_names[index2fname[detection[0]]]['path'])\n",
    "    x_min, y_min, x_max, y_max = [int(i) for i in detection[3:]]\n",
    "\n",
    "    crop_img = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    prediction = classify_crop(crop_img=crop_img, model=resnet50_resize, \n",
    "                    transform=data_transform_resize)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    # Вызываем классификатор\n",
    "    if classify_crop(crop_img=crop_img, model=resnet50_resize, \n",
    "                    transform=data_transform_resize) > 0.5:\n",
    "        detections_after_classifier.append(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 297\n",
      "recall 0.871\n",
      "precision 0.675\n",
      "f_score 0.761\n"
     ]
    }
   ],
   "source": [
    "print_main_metrics(*calc_tp_fp_fn(ground_truths, detections_after_classifier, iou_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 482/482 [02:52<00:00,  2.79it/s]\n"
     ]
    }
   ],
   "source": [
    "detections_after_classifier = []\n",
    "\n",
    "for detection in tqdm(detections):\n",
    "\n",
    "    img = cv2.imread(f_names[index2fname[detection[0]]]['path'])\n",
    "    x_min, y_min, x_max, y_max = [int(i) for i in detection[3:]]\n",
    "\n",
    "    crop_img = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    \n",
    "    # Вызываем классификатор\n",
    "    if classify_crop(crop_img=crop_img, model=resnet50_padding, #what the hell?\n",
    "                     transform=data_transform_padding) > 0.5:\n",
    "        detections_after_classifier.append(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 297\n",
      "recall 0.871\n",
      "precision 0.632\n",
      "f_score 0.732\n"
     ]
    }
   ],
   "source": [
    "print_main_metrics(*calc_tp_fp_fn(ground_truths, detections_after_classifier, iou_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_crop(crop_img, model=None, knn=None, transform=None):\n",
    "    model.eval()\n",
    "    \n",
    "    im = Image.fromarray(np.uint8(crop_img[:, :, [2, 1, 0]]))\n",
    "    transformed_image = transform(im)\n",
    "    transformed_image = transformed_image.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(transformed_image)\n",
    "    prediction = knn.predict(outputs.cpu().numpy())\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 482/482 [04:31<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "detections_after_classifier = []\n",
    "predictions = []\n",
    "\n",
    "for detection in tqdm(detections):\n",
    "\n",
    "    img = cv2.imread(f_names[index2fname[detection[0]]]['path'])\n",
    "    x_min, y_min, x_max, y_max = [int(i) for i in detection[3:]]\n",
    "\n",
    "    crop_img = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    prediction = classify_crop(crop_img=crop_img, model=resnet50_knn, knn=knn,\n",
    "                    transform=data_transform_resize)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    # Вызываем классификатор\n",
    "    if classify_crop(crop_img=crop_img, model=resnet50_knn, knn=knn,\n",
    "                    transform=data_transform_resize) > 0.5:\n",
    "        detections_after_classifier.append(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 297\n",
      "recall 0.871\n",
      "precision 0.616\n",
      "f_score 0.722\n"
     ]
    }
   ],
   "source": [
    "print_main_metrics(*calc_tp_fp_fn(ground_truths, detections_after_classifier, iou_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 482/482 [04:33<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "detections_after_classifier = []\n",
    "predictions = []\n",
    "\n",
    "for detection in tqdm(detections):\n",
    "\n",
    "    img = cv2.imread(f_names[index2fname[detection[0]]]['path'])\n",
    "    x_min, y_min, x_max, y_max = [int(i) for i in detection[3:]]\n",
    "\n",
    "    crop_img = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    prediction = classify_crop(crop_img=crop_img, model=resnet50_resize_knn, knn=knn,\n",
    "                    transform=data_transform_resize)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    # Вызываем классификатор\n",
    "    if classify_crop(crop_img=crop_img, model=resnet50_resize_knn, knn=knn,\n",
    "                    transform=data_transform_resize) > 0.5:\n",
    "        detections_after_classifier.append(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 297\n",
      "recall 0.871\n",
      "precision 0.624\n",
      "f_score 0.727\n"
     ]
    }
   ],
   "source": [
    "print_main_metrics(*calc_tp_fp_fn(ground_truths, detections_after_classifier, iou_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
